* what is faiss?

looks like it is some sort of in-mem db plus vector operations

* trasnformer: model
https://en.wikipedia.org/wiki/transformer_(machine_learning_model)
#+begin_quote
a transformer is a deep learning architecture, initially proposed in
2017, that relies on the parallel multi-head attention mechanism.[1]
it is notable for requiring less training time than previous recurrent
neural architectures, such as long short-term memory (lstm),[2] and
its later variation has been prevalently adopted for training large
language models on large (language) datasets, such as the wikipedia
corpus and common crawl, by virtue of the parallelized processing of
input sequence.[3]
#+end_quote

#+begin_quote
the transformer has had great success in natural language processing
(nlp), for example the tasks of machine translation and time series
prediction. many large language models such as gpt-2, gpt-3, gpt-4,
claude, bert, xlnet, roberta and chatgpt demonstrate the ability of
transformers to perform a wide variety of such nlp-related tasks, and
have the potential to find real-world applications.
#+end_quote

* misc / links

[[https://rustwasm.github.io/docs/book/reference/crates.html?highlight=crates#crates-you-should-know][Rust and WebAssembly: crates you should know]]

* hnsw-related

** knn

** nsw

** a-knn (approximate knn)

** proximity graphs

** lsh (locally-sensitive hashing)

** product quantization

** probabilistic skip list
https://en.wikipedia.org/wiki/Skip_list

** greedy algorithm

[[https://arxiv.org/pdf/1603.09320.pdf][Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs]]

[[https://github.com/jean-pierreBoth/hnswlib-rs/blob/master/tests/filtertest.rs][hnswlib-rs exmaple of filter]]

** knn

** nsw

** a-knn (approximate knn)

** proximity graphs

** lsh (locally-sensitive hashing)

** product quantization

** probabilistic skip list
https://en.wikipedia.org/wiki/Skip_list

** greedy algorithm

** Levenshtein distance
